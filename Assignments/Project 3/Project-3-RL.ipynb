{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### For the second part of this project we will be implementing a simple Q-Learning algorithm on an RL environment called Cart Pole. The idea of Q-Learning is to try to estimate the expected future reward or Q-value of taking a certain action. Then at any given step we take the action with the most expected future reward.\n",
    "\n",
    "### In reinforcement learning, we refer to algorithms that attempt to solve environments as \"agents\", so in this part of the project we will be making a Deep Q Network Agent that will solve the Cart Pole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/50/ed4a03d2be47ffd043be2ee514f329ce45d98a30fe2d1b9c61dea5a9d861/gym-0.10.5.tar.gz (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 566kB/s \n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/78/bc/de067ab2d700b91717dc5459d86a1877e2df31abfb90ab01a5a5a5ce30b4/tqdm-4.23.0-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /anaconda3/envs/py36/lib/python3.6/site-packages (from gym)\n",
      "Requirement already satisfied: requests>=2.0 in /anaconda3/envs/py36/lib/python3.6/site-packages (from gym)\n",
      "Requirement already satisfied: six in /anaconda3/envs/py36/lib/python3.6/site-packages (from gym)\n",
      "Collecting pyglet>=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 827kB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.0->gym)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.0->gym)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.0->gym)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.0->gym)\n",
      "Collecting future (from pyglet>=1.2.0->gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/2b/8d082ddfed935f3608cc61140df6dcbf0edea1bc3ab52fb6c29ae3e81e85/future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 1.0MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Running setup.py bdist_wheel for gym ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/chapatel/Library/Caches/pip/wheels/cb/14/71/f4ab006b1e6ff75c2b54985c2f98d0644fffe9c1dddc670925\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/chapatel/Library/Caches/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1a\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, pyglet, gym, tqdm\n",
      "Successfully installed future-0.16.0 gym-0.10.5 pyglet-1.3.2 tqdm-4.23.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create The DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from collections import deque\n",
    "import random\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, replay_size=1000, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, gamma=0.99):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.model = self.build_model()\n",
    "        self.replay_buffer = deque(maxlen=replay_size)\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # TODO: add 2 dense layers each with 32 neurons, the input dim to the first\n",
    "        # layer should be the state size, also add relu activations, for both these layers\n",
    "        # Then add another Dense layer with num_actions neurons.\n",
    "        # Then use model.compile to compile the model with mse loss and an Adam optimizer\n",
    "        # with learning rate 0.001.\n",
    "        model.add(Dense(32, input_dim = self.state_size))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(self.num_actions))\n",
    "        keras.optimizers.Adam(lr=0.001)\n",
    "        model.compile(optimizer = \"Adam\", loss=\"mse\")\n",
    "\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def action(self, state):\n",
    "        # Whenever a random number between 0 and 1 is less than epsilon we want to return\n",
    "        # a random action. This means that with probability epsilon we return a random action.\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "            #TODO: return random action here\n",
    "        # Now we want to use our model to get the q values\n",
    "        # HINT: we want to do prediction\n",
    "        \n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def add_to_replay_buffer(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_batch_from_replay(self, batch_size):\n",
    "        # if we don't have enough samples in our replay buffer just return\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return False\n",
    "        # TODO: randomly sample batch_size samples from the replay buffer\n",
    "        # hint: use random.sample\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_Qs = self.model.predict(next_state)[0]\n",
    "                # TODO: we want to add to our target GAMMA * max Q(next_state)\n",
    "                target += self.gamma * np.max(next_Qs)\n",
    "\n",
    "            # our target should only take into account the current action\n",
    "            # so we set all the Q values except the current action, to the \n",
    "            # current output of our model so that they get ignored in the loss function.\n",
    "            target_Qs = self.model.predict(state)\n",
    "            target_Qs[0][action] = target\n",
    "            self.model.fit(state, target_Qs, epochs=1, verbose=0)\n",
    "        \n",
    "        # Now we want to slowly decay how many random actions we take\n",
    "        # to do this we can multiply epsilon by our epsilon decay parameter\n",
    "        # each iteration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [34:59<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "done = False\n",
    "batch_size = 32\n",
    "num_episodes = 800\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    \n",
    "    for t in range(200):\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else 100\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        agent.add_to_replay_buffer(state, action, reward, next_state, done)\n",
    "        # TODO: add this sample to the replay buffer\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # TODO: train on a batch from the replay buffer\n",
    "        agent.train_batch_from_replay(batch_size)\n",
    "        if done: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "40.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "23.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "27.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "21.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "35.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "21.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "24.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "46.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "20.0\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "27.0\n"
     ]
    }
   ],
   "source": [
    "#TODO: set the agent's epsilon so that we don't take any random actions.\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    agent.episilon = -1\n",
    "    total_reward = 0\n",
    "    for t in range(200):\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = np.reshape(next_state, [1, agent.state_size])\n",
    "        # TODO: if you want to see the rendered version of your agent running\n",
    "        # uncomment this line\n",
    "        #env.render()\n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Writeup\n",
    "\n",
    "#### Now for the writeup portion write a paragraph of your understanding of how Deep Q Learning works.\n",
    "\n",
    "Q-learning uses a simple update rule to perform q-value iteration, which allows us to bypass the need to keep track of values, transition functions, and reward functions. We use Deep Q-Learning to approximate our Q-value function with the use of a Neural Network. We choose the neuron from out network that has the highest value and take an action corresponding to this neuron. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
